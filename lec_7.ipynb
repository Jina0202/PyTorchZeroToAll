{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lec_7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOskfqf9fgcM4U7dwA21JXe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jina0202/PyTorchZeroToAll/blob/main/lec_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfNMq9evbPfl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifNO0sbvbRpV"
      },
      "source": [
        "#lecture 8 DataLoader\n",
        "\n",
        "-previous data: Manual data feed\n",
        "read all data from file -> divide x, y data ->feed all data to the model\n",
        "\n",
        "**feeding all data is not good when using large data set\n",
        "\n",
        "==> Batch (batch size)\n",
        "\n",
        "-divide entire data sets into small batches (reduced batch size)\n",
        "\n",
        "-go throgh each batch at once -> gradient -> update weight\n",
        "\n",
        "-more formally, we can devide epoch, batch size, and iterations\n",
        "one epoch: entire data\n",
        "each batch covering one batch is one iteration\n",
        "\n",
        "-->How implement? how load branches? (DataLoader사용하면 됨)\n",
        "\n",
        "iterable data 만 필요함 --> batches 있으면 됨, \n",
        "\n",
        "//epoch, batch size, iterations, data loader 가 뭔지\n",
        "//csv.\n",
        "\n",
        "##custom dataloader\n",
        "1) get item\n",
        "\n",
        "2) what is size of the data\n",
        "\n",
        "==>\n",
        "\n",
        "1) download, read data etc\n",
        "\n",
        "2) return one item on the index\n",
        "\n",
        "3) return the data length\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THZOobju4jrH"
      },
      "source": [
        "from torch import from_numpy, tensor\n",
        "import numpy as np\n",
        "\n",
        "class DiabetesDataset(Dataset):\n",
        "    \"\"\" Diabetes dataset.\"\"\"\n",
        "       xy = np.loadtxt('./data/diabetes.csv.gz',\n",
        "                        delimiter=',', dtype=np.float32)\n",
        "        self.len = xy.shape[0]\n",
        "        self.x_data = from_numpy(xy[:, 0:-1])\n",
        "        self.y_data = from_numpy(xy[:, [-1]])\n",
        "      def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "        inputs, labels = data\n",
        "\n",
        "        # wrap them in Variable\n",
        "        inputs, labels = tensor(inputs), tensor(labels)\n",
        "        print(f'Epoch: {i} | Inputs {inputs.data} | Labels {labels.data}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sthixcek5dGF"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QidcyQzl5fco",
        "outputId": "7cfc88dd-1a3a-4ae8-cb63-07e43a09569c"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn, from_numpy, optim\n",
        "import numpy as np\n",
        "\n",
        "class DiabetesDataset(Dataset):\n",
        "\n",
        "  \n",
        "  def __init__(self):\n",
        "    xy = np.loadtxt('/content/diabetes.csv', delimiter=',', dtype=np.float32)\n",
        "    self.len=xy.shape[0]\n",
        "    self.x_data = from_numpy(xy[:, 0:-1])\n",
        "    self.y_data = from_numpy(xy[:, [-1]])\n",
        "  def __getitem__(self, index):\n",
        "    return self.x_data[index],self.y_data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "\n",
        "dataset=DiabetesDataset()\n",
        "train_loader=DataLoader(dataset=dataset,\n",
        "                        batch_size=32,\n",
        "                        shuffle=True,\n",
        "                        num_workers=2)\n",
        "\n",
        "class Model(nn.Module):\n",
        "  #아무렇게나 이름 지음->Model, Model은 torch.nn.Module의 subclass\n",
        "  #two function: __init__(), forward()\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    In the constructor wew instantiate tow nn.Linear module\n",
        "    \"\"\"\n",
        "    super(Model,self).__init__() #going to call super in it, do some initializaion task or creat element,some component\n",
        "    self.l1 = nn.Linear(8, 6)\n",
        "    self.l2 = nn.Linear(6, 4)\n",
        "    self.l3 = nn.Linear(4, 1)\n",
        "\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "  \n",
        "  def forward(self,x): #not going to use our own weight, but use the block we initialized in it\n",
        "    out1=self.sigmoid(self.l1(x))\n",
        "    out2=self.sigmoid(self.l2(out1))\n",
        "    y_pred=self.sigmoid(self.l3(out2))\n",
        "    return y_pred\n",
        "\n",
        "model=Model()\n",
        "###step 2\n",
        "#Construct our loss funtion and an Optimizer. The call to model.parameters\n",
        "#in the SGD constructor will contain the learnable parameters of the two\n",
        "#nn.Linear modules whoch are members of the model.\n",
        "criterion = nn.BCELoss(reduction='sum') #라이브러리\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1) #알고리즘 compute data same times-앞 영상방식 zip들어간 방식보다 효율적 \n",
        "\n",
        "###step 3\n",
        "#Training loop\n",
        "for epoch in range(2):\n",
        " for i, data in enumerate(train_loader,0):\n",
        "   inputs,labels=data\n",
        "   inputs,labels=Variable(inputs),Variable(labels)\n",
        "   y_pred=model(inputs)\n",
        "   loss=criterion(y_pred,labels)\n",
        "   print(f'Epoch {epoch + 1} | Batch: {i+1} | Loss: {loss.item():.4f}')\n",
        "\n",
        "   optimizer.zero_grad() #initialize all the gradients\n",
        "  #update variable <</////////how??\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 | Batch: 1 | Loss: 24.7266\n",
            "Epoch 1 | Batch: 2 | Loss: 24.7329\n",
            "Epoch 1 | Batch: 3 | Loss: 26.7360\n",
            "Epoch 1 | Batch: 4 | Loss: 21.1903\n",
            "Epoch 1 | Batch: 5 | Loss: 24.7252\n",
            "Epoch 1 | Batch: 6 | Loss: 25.2278\n",
            "Epoch 1 | Batch: 7 | Loss: 26.7411\n",
            "Epoch 1 | Batch: 8 | Loss: 27.2584\n",
            "Epoch 1 | Batch: 9 | Loss: 24.7223\n",
            "Epoch 1 | Batch: 10 | Loss: 25.2321\n",
            "Epoch 1 | Batch: 11 | Loss: 24.7251\n",
            "Epoch 1 | Batch: 12 | Loss: 26.7435\n",
            "Epoch 1 | Batch: 13 | Loss: 26.7422\n",
            "Epoch 1 | Batch: 14 | Loss: 26.2439\n",
            "Epoch 1 | Batch: 15 | Loss: 24.7257\n",
            "Epoch 1 | Batch: 16 | Loss: 24.7184\n",
            "Epoch 1 | Batch: 17 | Loss: 27.2436\n",
            "Epoch 1 | Batch: 18 | Loss: 25.7372\n",
            "Epoch 1 | Batch: 19 | Loss: 24.7192\n",
            "Epoch 1 | Batch: 20 | Loss: 27.7586\n",
            "Epoch 1 | Batch: 21 | Loss: 25.7347\n",
            "Epoch 1 | Batch: 22 | Loss: 27.7558\n",
            "Epoch 1 | Batch: 23 | Loss: 25.7341\n",
            "Epoch 1 | Batch: 24 | Loss: 19.4679\n",
            "Epoch 2 | Batch: 1 | Loss: 26.7434\n",
            "Epoch 2 | Batch: 2 | Loss: 28.2659\n",
            "Epoch 2 | Batch: 3 | Loss: 25.7355\n",
            "Epoch 2 | Batch: 4 | Loss: 25.2354\n",
            "Epoch 2 | Batch: 5 | Loss: 26.2380\n",
            "Epoch 2 | Batch: 6 | Loss: 25.7281\n",
            "Epoch 2 | Batch: 7 | Loss: 25.2414\n",
            "Epoch 2 | Batch: 8 | Loss: 25.7362\n",
            "Epoch 2 | Batch: 9 | Loss: 24.7230\n",
            "Epoch 2 | Batch: 10 | Loss: 25.7433\n",
            "Epoch 2 | Batch: 11 | Loss: 24.2256\n",
            "Epoch 2 | Batch: 12 | Loss: 26.7420\n",
            "Epoch 2 | Batch: 13 | Loss: 25.2189\n",
            "Epoch 2 | Batch: 14 | Loss: 24.2189\n",
            "Epoch 2 | Batch: 15 | Loss: 26.2385\n",
            "Epoch 2 | Batch: 16 | Loss: 27.2466\n",
            "Epoch 2 | Batch: 17 | Loss: 25.2219\n",
            "Epoch 2 | Batch: 18 | Loss: 24.7309\n",
            "Epoch 2 | Batch: 19 | Loss: 27.7540\n",
            "Epoch 2 | Batch: 20 | Loss: 24.7276\n",
            "Epoch 2 | Batch: 21 | Loss: 25.2241\n",
            "Epoch 2 | Batch: 22 | Loss: 25.2219\n",
            "Epoch 2 | Batch: 23 | Loss: 26.7455\n",
            "Epoch 2 | Batch: 24 | Loss: 16.4362\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUMhD8i186U6",
        "outputId": "6038808a-626e-4090-9c40-54f090c08b58"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1Du6Rpvfll6RyYCA9Ptrve36cu4qGFADR"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Du6Rpvfll6RyYCA9Ptrve36cu4qGFADR\n",
            "To: /content/diabetes.csv\n",
            "\r  0% 0.00/53.4k [00:00<?, ?B/s]\r100% 53.4k/53.4k [00:00<00:00, 20.1MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}