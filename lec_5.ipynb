{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " lec_5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN85YWKpPLez+BaGF2wy31o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jina0202/PyTorchZeroToAll/blob/main/lec_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIi_d1V5MwH0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXelypltM0Tn"
      },
      "source": [
        "#lecture 6 Logistic Regression\n",
        "\n",
        "-our example was s, y both real number, but binary prediction(0,1) is very useful!\n",
        "\n",
        "solutions:\n",
        "\n",
        "1) plug in Sigmoid function\n",
        "\n",
        "y^ value: 0~1 (graph)\n",
        "\n",
        "1: y^>0.5\n",
        "\n",
        "0:otherwise\n",
        "\n",
        "loss --> cross entropy loss\n",
        "\n",
        "MSELoss --> BCELoss\n",
        "\n",
        "==> use y_data as 0, 1\n",
        "F.Sigmoid()\n",
        "BCELoss()\n",
        "value 1--> output compared with 0.5\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uv_MdjuQ55R",
        "outputId": "6c1c097a-87e0-477d-96d6-2edccbf5d6c0"
      },
      "source": [
        "###step 1\n",
        "from torch import tensor\n",
        "from torch import nn\n",
        "from torch import sigmoid\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "#data definition (3x1 MATRIX)\n",
        "#3data, each data has 1 value x --> 일반화 nx1 MATRIX\n",
        "#changed the form into MATRIX\n",
        "# Training data and ground truth\n",
        "x_data = tensor([[1.0], [2.0], [3.0], [4.0]])\n",
        "y_data = tensor([[0.], [0.], [1.], [1.]])\n",
        "\n",
        "class Model(nn.Module):\n",
        "  #아무렇게나 이름 지음->Model, Model은 torch.nn.Module의 subclass\n",
        "  #two function: __init__(), forward()\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    In the constructor wew instantiate tow nn.Linear module\n",
        "    \"\"\"\n",
        "    super(Model,self).__init__() #going to call super in it, do some initializaion task or creat element,some component\n",
        "    self.linear = nn.Linear(1, 1) #make one linear block: (input,output) input, output is 1 bc x, y value is one each\n",
        "  \n",
        "  def forward(self,x): #not going to use our own weight, but use the block we initialized in it\n",
        "    y_pred = sigmoid(self.linear(x))\n",
        "    return y_pred\n",
        "model=Model()\n",
        "###step 2\n",
        "#Construct our loss funtion and an Optimizer. The call to model.parameters\n",
        "#in the SGD constructor will contain the learnable parameters of the two\n",
        "#nn.Linear modules whoch are members of the model.\n",
        "criterion = nn.BCELoss(reduction='mean') #라이브러리\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01) #알고리즘 compute data same times-앞 영상방식 zip들어간 방식보다 효율적 \n",
        "\n",
        "###step 3\n",
        "#Training loop\n",
        "for epoch in range(500):\n",
        "  #forward poss: compute predicted y by passing x to the model\n",
        "  y_pred = model(x_data) #s_data: x_data=matrix\n",
        "\n",
        "  #Compute and print loss\n",
        "  loss=criterion(y_pred, y_data) #MSE계산\n",
        "  print(f'Epoch {epoch + 1}/1000 | Loss: {loss.item():.4f}')\n",
        "\n",
        "  #Zero gradients, perform a backward pass, and update the weights\n",
        "  optimizer.zero_grad() #initialize all the gradients\n",
        "  loss.backward()\n",
        "  optimizer.step() #update variable <</////////how??\n",
        "\n",
        "  #After Training\n",
        "   \n",
        "print(f'\\nLet\\'s predict the hours need to score above 50%\\n{\"=\" * 50}')\n",
        "hour_var = model(tensor([[1.0]]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() > 0.5}')\n",
        "hour_var = model(tensor([[7.0]]))\n",
        "print(f'Prediction after 7 hours of training: {hour_var.item():.4f} | Above 50%: { hour_var.item() > 0.5}')\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000 | Loss: 0.7809\n",
            "Epoch 2/1000 | Loss: 0.7704\n",
            "Epoch 3/1000 | Loss: 0.7603\n",
            "Epoch 4/1000 | Loss: 0.7506\n",
            "Epoch 5/1000 | Loss: 0.7412\n",
            "Epoch 6/1000 | Loss: 0.7322\n",
            "Epoch 7/1000 | Loss: 0.7234\n",
            "Epoch 8/1000 | Loss: 0.7150\n",
            "Epoch 9/1000 | Loss: 0.7069\n",
            "Epoch 10/1000 | Loss: 0.6991\n",
            "Epoch 11/1000 | Loss: 0.6916\n",
            "Epoch 12/1000 | Loss: 0.6843\n",
            "Epoch 13/1000 | Loss: 0.6774\n",
            "Epoch 14/1000 | Loss: 0.6707\n",
            "Epoch 15/1000 | Loss: 0.6643\n",
            "Epoch 16/1000 | Loss: 0.6581\n",
            "Epoch 17/1000 | Loss: 0.6521\n",
            "Epoch 18/1000 | Loss: 0.6464\n",
            "Epoch 19/1000 | Loss: 0.6409\n",
            "Epoch 20/1000 | Loss: 0.6356\n",
            "Epoch 21/1000 | Loss: 0.6305\n",
            "Epoch 22/1000 | Loss: 0.6256\n",
            "Epoch 23/1000 | Loss: 0.6209\n",
            "Epoch 24/1000 | Loss: 0.6164\n",
            "Epoch 25/1000 | Loss: 0.6121\n",
            "Epoch 26/1000 | Loss: 0.6079\n",
            "Epoch 27/1000 | Loss: 0.6039\n",
            "Epoch 28/1000 | Loss: 0.6001\n",
            "Epoch 29/1000 | Loss: 0.5964\n",
            "Epoch 30/1000 | Loss: 0.5929\n",
            "Epoch 31/1000 | Loss: 0.5895\n",
            "Epoch 32/1000 | Loss: 0.5862\n",
            "Epoch 33/1000 | Loss: 0.5830\n",
            "Epoch 34/1000 | Loss: 0.5800\n",
            "Epoch 35/1000 | Loss: 0.5771\n",
            "Epoch 36/1000 | Loss: 0.5743\n",
            "Epoch 37/1000 | Loss: 0.5716\n",
            "Epoch 38/1000 | Loss: 0.5690\n",
            "Epoch 39/1000 | Loss: 0.5665\n",
            "Epoch 40/1000 | Loss: 0.5641\n",
            "Epoch 41/1000 | Loss: 0.5618\n",
            "Epoch 42/1000 | Loss: 0.5596\n",
            "Epoch 43/1000 | Loss: 0.5575\n",
            "Epoch 44/1000 | Loss: 0.5554\n",
            "Epoch 45/1000 | Loss: 0.5534\n",
            "Epoch 46/1000 | Loss: 0.5515\n",
            "Epoch 47/1000 | Loss: 0.5497\n",
            "Epoch 48/1000 | Loss: 0.5479\n",
            "Epoch 49/1000 | Loss: 0.5462\n",
            "Epoch 50/1000 | Loss: 0.5446\n",
            "Epoch 51/1000 | Loss: 0.5430\n",
            "Epoch 52/1000 | Loss: 0.5415\n",
            "Epoch 53/1000 | Loss: 0.5400\n",
            "Epoch 54/1000 | Loss: 0.5386\n",
            "Epoch 55/1000 | Loss: 0.5372\n",
            "Epoch 56/1000 | Loss: 0.5359\n",
            "Epoch 57/1000 | Loss: 0.5346\n",
            "Epoch 58/1000 | Loss: 0.5334\n",
            "Epoch 59/1000 | Loss: 0.5322\n",
            "Epoch 60/1000 | Loss: 0.5310\n",
            "Epoch 61/1000 | Loss: 0.5299\n",
            "Epoch 62/1000 | Loss: 0.5288\n",
            "Epoch 63/1000 | Loss: 0.5278\n",
            "Epoch 64/1000 | Loss: 0.5268\n",
            "Epoch 65/1000 | Loss: 0.5258\n",
            "Epoch 66/1000 | Loss: 0.5249\n",
            "Epoch 67/1000 | Loss: 0.5240\n",
            "Epoch 68/1000 | Loss: 0.5231\n",
            "Epoch 69/1000 | Loss: 0.5222\n",
            "Epoch 70/1000 | Loss: 0.5214\n",
            "Epoch 71/1000 | Loss: 0.5206\n",
            "Epoch 72/1000 | Loss: 0.5198\n",
            "Epoch 73/1000 | Loss: 0.5191\n",
            "Epoch 74/1000 | Loss: 0.5183\n",
            "Epoch 75/1000 | Loss: 0.5176\n",
            "Epoch 76/1000 | Loss: 0.5169\n",
            "Epoch 77/1000 | Loss: 0.5163\n",
            "Epoch 78/1000 | Loss: 0.5156\n",
            "Epoch 79/1000 | Loss: 0.5150\n",
            "Epoch 80/1000 | Loss: 0.5143\n",
            "Epoch 81/1000 | Loss: 0.5137\n",
            "Epoch 82/1000 | Loss: 0.5132\n",
            "Epoch 83/1000 | Loss: 0.5126\n",
            "Epoch 84/1000 | Loss: 0.5120\n",
            "Epoch 85/1000 | Loss: 0.5115\n",
            "Epoch 86/1000 | Loss: 0.5110\n",
            "Epoch 87/1000 | Loss: 0.5105\n",
            "Epoch 88/1000 | Loss: 0.5100\n",
            "Epoch 89/1000 | Loss: 0.5095\n",
            "Epoch 90/1000 | Loss: 0.5090\n",
            "Epoch 91/1000 | Loss: 0.5085\n",
            "Epoch 92/1000 | Loss: 0.5081\n",
            "Epoch 93/1000 | Loss: 0.5076\n",
            "Epoch 94/1000 | Loss: 0.5072\n",
            "Epoch 95/1000 | Loss: 0.5068\n",
            "Epoch 96/1000 | Loss: 0.5064\n",
            "Epoch 97/1000 | Loss: 0.5060\n",
            "Epoch 98/1000 | Loss: 0.5056\n",
            "Epoch 99/1000 | Loss: 0.5052\n",
            "Epoch 100/1000 | Loss: 0.5048\n",
            "Epoch 101/1000 | Loss: 0.5045\n",
            "Epoch 102/1000 | Loss: 0.5041\n",
            "Epoch 103/1000 | Loss: 0.5037\n",
            "Epoch 104/1000 | Loss: 0.5034\n",
            "Epoch 105/1000 | Loss: 0.5030\n",
            "Epoch 106/1000 | Loss: 0.5027\n",
            "Epoch 107/1000 | Loss: 0.5024\n",
            "Epoch 108/1000 | Loss: 0.5021\n",
            "Epoch 109/1000 | Loss: 0.5017\n",
            "Epoch 110/1000 | Loss: 0.5014\n",
            "Epoch 111/1000 | Loss: 0.5011\n",
            "Epoch 112/1000 | Loss: 0.5008\n",
            "Epoch 113/1000 | Loss: 0.5005\n",
            "Epoch 114/1000 | Loss: 0.5002\n",
            "Epoch 115/1000 | Loss: 0.4999\n",
            "Epoch 116/1000 | Loss: 0.4996\n",
            "Epoch 117/1000 | Loss: 0.4994\n",
            "Epoch 118/1000 | Loss: 0.4991\n",
            "Epoch 119/1000 | Loss: 0.4988\n",
            "Epoch 120/1000 | Loss: 0.4986\n",
            "Epoch 121/1000 | Loss: 0.4983\n",
            "Epoch 122/1000 | Loss: 0.4980\n",
            "Epoch 123/1000 | Loss: 0.4978\n",
            "Epoch 124/1000 | Loss: 0.4975\n",
            "Epoch 125/1000 | Loss: 0.4973\n",
            "Epoch 126/1000 | Loss: 0.4970\n",
            "Epoch 127/1000 | Loss: 0.4968\n",
            "Epoch 128/1000 | Loss: 0.4965\n",
            "Epoch 129/1000 | Loss: 0.4963\n",
            "Epoch 130/1000 | Loss: 0.4961\n",
            "Epoch 131/1000 | Loss: 0.4958\n",
            "Epoch 132/1000 | Loss: 0.4956\n",
            "Epoch 133/1000 | Loss: 0.4954\n",
            "Epoch 134/1000 | Loss: 0.4952\n",
            "Epoch 135/1000 | Loss: 0.4949\n",
            "Epoch 136/1000 | Loss: 0.4947\n",
            "Epoch 137/1000 | Loss: 0.4945\n",
            "Epoch 138/1000 | Loss: 0.4943\n",
            "Epoch 139/1000 | Loss: 0.4941\n",
            "Epoch 140/1000 | Loss: 0.4938\n",
            "Epoch 141/1000 | Loss: 0.4936\n",
            "Epoch 142/1000 | Loss: 0.4934\n",
            "Epoch 143/1000 | Loss: 0.4932\n",
            "Epoch 144/1000 | Loss: 0.4930\n",
            "Epoch 145/1000 | Loss: 0.4928\n",
            "Epoch 146/1000 | Loss: 0.4926\n",
            "Epoch 147/1000 | Loss: 0.4924\n",
            "Epoch 148/1000 | Loss: 0.4922\n",
            "Epoch 149/1000 | Loss: 0.4920\n",
            "Epoch 150/1000 | Loss: 0.4918\n",
            "Epoch 151/1000 | Loss: 0.4916\n",
            "Epoch 152/1000 | Loss: 0.4914\n",
            "Epoch 153/1000 | Loss: 0.4912\n",
            "Epoch 154/1000 | Loss: 0.4910\n",
            "Epoch 155/1000 | Loss: 0.4908\n",
            "Epoch 156/1000 | Loss: 0.4906\n",
            "Epoch 157/1000 | Loss: 0.4905\n",
            "Epoch 158/1000 | Loss: 0.4903\n",
            "Epoch 159/1000 | Loss: 0.4901\n",
            "Epoch 160/1000 | Loss: 0.4899\n",
            "Epoch 161/1000 | Loss: 0.4897\n",
            "Epoch 162/1000 | Loss: 0.4895\n",
            "Epoch 163/1000 | Loss: 0.4893\n",
            "Epoch 164/1000 | Loss: 0.4892\n",
            "Epoch 165/1000 | Loss: 0.4890\n",
            "Epoch 166/1000 | Loss: 0.4888\n",
            "Epoch 167/1000 | Loss: 0.4886\n",
            "Epoch 168/1000 | Loss: 0.4884\n",
            "Epoch 169/1000 | Loss: 0.4883\n",
            "Epoch 170/1000 | Loss: 0.4881\n",
            "Epoch 171/1000 | Loss: 0.4879\n",
            "Epoch 172/1000 | Loss: 0.4877\n",
            "Epoch 173/1000 | Loss: 0.4876\n",
            "Epoch 174/1000 | Loss: 0.4874\n",
            "Epoch 175/1000 | Loss: 0.4872\n",
            "Epoch 176/1000 | Loss: 0.4870\n",
            "Epoch 177/1000 | Loss: 0.4869\n",
            "Epoch 178/1000 | Loss: 0.4867\n",
            "Epoch 179/1000 | Loss: 0.4865\n",
            "Epoch 180/1000 | Loss: 0.4864\n",
            "Epoch 181/1000 | Loss: 0.4862\n",
            "Epoch 182/1000 | Loss: 0.4860\n",
            "Epoch 183/1000 | Loss: 0.4858\n",
            "Epoch 184/1000 | Loss: 0.4857\n",
            "Epoch 185/1000 | Loss: 0.4855\n",
            "Epoch 186/1000 | Loss: 0.4853\n",
            "Epoch 187/1000 | Loss: 0.4852\n",
            "Epoch 188/1000 | Loss: 0.4850\n",
            "Epoch 189/1000 | Loss: 0.4848\n",
            "Epoch 190/1000 | Loss: 0.4847\n",
            "Epoch 191/1000 | Loss: 0.4845\n",
            "Epoch 192/1000 | Loss: 0.4843\n",
            "Epoch 193/1000 | Loss: 0.4842\n",
            "Epoch 194/1000 | Loss: 0.4840\n",
            "Epoch 195/1000 | Loss: 0.4838\n",
            "Epoch 196/1000 | Loss: 0.4837\n",
            "Epoch 197/1000 | Loss: 0.4835\n",
            "Epoch 198/1000 | Loss: 0.4834\n",
            "Epoch 199/1000 | Loss: 0.4832\n",
            "Epoch 200/1000 | Loss: 0.4830\n",
            "Epoch 201/1000 | Loss: 0.4829\n",
            "Epoch 202/1000 | Loss: 0.4827\n",
            "Epoch 203/1000 | Loss: 0.4825\n",
            "Epoch 204/1000 | Loss: 0.4824\n",
            "Epoch 205/1000 | Loss: 0.4822\n",
            "Epoch 206/1000 | Loss: 0.4821\n",
            "Epoch 207/1000 | Loss: 0.4819\n",
            "Epoch 208/1000 | Loss: 0.4817\n",
            "Epoch 209/1000 | Loss: 0.4816\n",
            "Epoch 210/1000 | Loss: 0.4814\n",
            "Epoch 211/1000 | Loss: 0.4813\n",
            "Epoch 212/1000 | Loss: 0.4811\n",
            "Epoch 213/1000 | Loss: 0.4809\n",
            "Epoch 214/1000 | Loss: 0.4808\n",
            "Epoch 215/1000 | Loss: 0.4806\n",
            "Epoch 216/1000 | Loss: 0.4805\n",
            "Epoch 217/1000 | Loss: 0.4803\n",
            "Epoch 218/1000 | Loss: 0.4801\n",
            "Epoch 219/1000 | Loss: 0.4800\n",
            "Epoch 220/1000 | Loss: 0.4798\n",
            "Epoch 221/1000 | Loss: 0.4797\n",
            "Epoch 222/1000 | Loss: 0.4795\n",
            "Epoch 223/1000 | Loss: 0.4794\n",
            "Epoch 224/1000 | Loss: 0.4792\n",
            "Epoch 225/1000 | Loss: 0.4790\n",
            "Epoch 226/1000 | Loss: 0.4789\n",
            "Epoch 227/1000 | Loss: 0.4787\n",
            "Epoch 228/1000 | Loss: 0.4786\n",
            "Epoch 229/1000 | Loss: 0.4784\n",
            "Epoch 230/1000 | Loss: 0.4783\n",
            "Epoch 231/1000 | Loss: 0.4781\n",
            "Epoch 232/1000 | Loss: 0.4780\n",
            "Epoch 233/1000 | Loss: 0.4778\n",
            "Epoch 234/1000 | Loss: 0.4776\n",
            "Epoch 235/1000 | Loss: 0.4775\n",
            "Epoch 236/1000 | Loss: 0.4773\n",
            "Epoch 237/1000 | Loss: 0.4772\n",
            "Epoch 238/1000 | Loss: 0.4770\n",
            "Epoch 239/1000 | Loss: 0.4769\n",
            "Epoch 240/1000 | Loss: 0.4767\n",
            "Epoch 241/1000 | Loss: 0.4766\n",
            "Epoch 242/1000 | Loss: 0.4764\n",
            "Epoch 243/1000 | Loss: 0.4763\n",
            "Epoch 244/1000 | Loss: 0.4761\n",
            "Epoch 245/1000 | Loss: 0.4760\n",
            "Epoch 246/1000 | Loss: 0.4758\n",
            "Epoch 247/1000 | Loss: 0.4757\n",
            "Epoch 248/1000 | Loss: 0.4755\n",
            "Epoch 249/1000 | Loss: 0.4753\n",
            "Epoch 250/1000 | Loss: 0.4752\n",
            "Epoch 251/1000 | Loss: 0.4750\n",
            "Epoch 252/1000 | Loss: 0.4749\n",
            "Epoch 253/1000 | Loss: 0.4747\n",
            "Epoch 254/1000 | Loss: 0.4746\n",
            "Epoch 255/1000 | Loss: 0.4744\n",
            "Epoch 256/1000 | Loss: 0.4743\n",
            "Epoch 257/1000 | Loss: 0.4741\n",
            "Epoch 258/1000 | Loss: 0.4740\n",
            "Epoch 259/1000 | Loss: 0.4738\n",
            "Epoch 260/1000 | Loss: 0.4737\n",
            "Epoch 261/1000 | Loss: 0.4735\n",
            "Epoch 262/1000 | Loss: 0.4734\n",
            "Epoch 263/1000 | Loss: 0.4732\n",
            "Epoch 264/1000 | Loss: 0.4731\n",
            "Epoch 265/1000 | Loss: 0.4729\n",
            "Epoch 266/1000 | Loss: 0.4728\n",
            "Epoch 267/1000 | Loss: 0.4726\n",
            "Epoch 268/1000 | Loss: 0.4725\n",
            "Epoch 269/1000 | Loss: 0.4723\n",
            "Epoch 270/1000 | Loss: 0.4722\n",
            "Epoch 271/1000 | Loss: 0.4720\n",
            "Epoch 272/1000 | Loss: 0.4719\n",
            "Epoch 273/1000 | Loss: 0.4717\n",
            "Epoch 274/1000 | Loss: 0.4716\n",
            "Epoch 275/1000 | Loss: 0.4714\n",
            "Epoch 276/1000 | Loss: 0.4713\n",
            "Epoch 277/1000 | Loss: 0.4711\n",
            "Epoch 278/1000 | Loss: 0.4710\n",
            "Epoch 279/1000 | Loss: 0.4708\n",
            "Epoch 280/1000 | Loss: 0.4707\n",
            "Epoch 281/1000 | Loss: 0.4705\n",
            "Epoch 282/1000 | Loss: 0.4704\n",
            "Epoch 283/1000 | Loss: 0.4702\n",
            "Epoch 284/1000 | Loss: 0.4701\n",
            "Epoch 285/1000 | Loss: 0.4699\n",
            "Epoch 286/1000 | Loss: 0.4698\n",
            "Epoch 287/1000 | Loss: 0.4697\n",
            "Epoch 288/1000 | Loss: 0.4695\n",
            "Epoch 289/1000 | Loss: 0.4694\n",
            "Epoch 290/1000 | Loss: 0.4692\n",
            "Epoch 291/1000 | Loss: 0.4691\n",
            "Epoch 292/1000 | Loss: 0.4689\n",
            "Epoch 293/1000 | Loss: 0.4688\n",
            "Epoch 294/1000 | Loss: 0.4686\n",
            "Epoch 295/1000 | Loss: 0.4685\n",
            "Epoch 296/1000 | Loss: 0.4683\n",
            "Epoch 297/1000 | Loss: 0.4682\n",
            "Epoch 298/1000 | Loss: 0.4680\n",
            "Epoch 299/1000 | Loss: 0.4679\n",
            "Epoch 300/1000 | Loss: 0.4677\n",
            "Epoch 301/1000 | Loss: 0.4676\n",
            "Epoch 302/1000 | Loss: 0.4674\n",
            "Epoch 303/1000 | Loss: 0.4673\n",
            "Epoch 304/1000 | Loss: 0.4672\n",
            "Epoch 305/1000 | Loss: 0.4670\n",
            "Epoch 306/1000 | Loss: 0.4669\n",
            "Epoch 307/1000 | Loss: 0.4667\n",
            "Epoch 308/1000 | Loss: 0.4666\n",
            "Epoch 309/1000 | Loss: 0.4664\n",
            "Epoch 310/1000 | Loss: 0.4663\n",
            "Epoch 311/1000 | Loss: 0.4661\n",
            "Epoch 312/1000 | Loss: 0.4660\n",
            "Epoch 313/1000 | Loss: 0.4658\n",
            "Epoch 314/1000 | Loss: 0.4657\n",
            "Epoch 315/1000 | Loss: 0.4656\n",
            "Epoch 316/1000 | Loss: 0.4654\n",
            "Epoch 317/1000 | Loss: 0.4653\n",
            "Epoch 318/1000 | Loss: 0.4651\n",
            "Epoch 319/1000 | Loss: 0.4650\n",
            "Epoch 320/1000 | Loss: 0.4648\n",
            "Epoch 321/1000 | Loss: 0.4647\n",
            "Epoch 322/1000 | Loss: 0.4645\n",
            "Epoch 323/1000 | Loss: 0.4644\n",
            "Epoch 324/1000 | Loss: 0.4643\n",
            "Epoch 325/1000 | Loss: 0.4641\n",
            "Epoch 326/1000 | Loss: 0.4640\n",
            "Epoch 327/1000 | Loss: 0.4638\n",
            "Epoch 328/1000 | Loss: 0.4637\n",
            "Epoch 329/1000 | Loss: 0.4635\n",
            "Epoch 330/1000 | Loss: 0.4634\n",
            "Epoch 331/1000 | Loss: 0.4633\n",
            "Epoch 332/1000 | Loss: 0.4631\n",
            "Epoch 333/1000 | Loss: 0.4630\n",
            "Epoch 334/1000 | Loss: 0.4628\n",
            "Epoch 335/1000 | Loss: 0.4627\n",
            "Epoch 336/1000 | Loss: 0.4625\n",
            "Epoch 337/1000 | Loss: 0.4624\n",
            "Epoch 338/1000 | Loss: 0.4623\n",
            "Epoch 339/1000 | Loss: 0.4621\n",
            "Epoch 340/1000 | Loss: 0.4620\n",
            "Epoch 341/1000 | Loss: 0.4618\n",
            "Epoch 342/1000 | Loss: 0.4617\n",
            "Epoch 343/1000 | Loss: 0.4615\n",
            "Epoch 344/1000 | Loss: 0.4614\n",
            "Epoch 345/1000 | Loss: 0.4613\n",
            "Epoch 346/1000 | Loss: 0.4611\n",
            "Epoch 347/1000 | Loss: 0.4610\n",
            "Epoch 348/1000 | Loss: 0.4608\n",
            "Epoch 349/1000 | Loss: 0.4607\n",
            "Epoch 350/1000 | Loss: 0.4605\n",
            "Epoch 351/1000 | Loss: 0.4604\n",
            "Epoch 352/1000 | Loss: 0.4603\n",
            "Epoch 353/1000 | Loss: 0.4601\n",
            "Epoch 354/1000 | Loss: 0.4600\n",
            "Epoch 355/1000 | Loss: 0.4598\n",
            "Epoch 356/1000 | Loss: 0.4597\n",
            "Epoch 357/1000 | Loss: 0.4596\n",
            "Epoch 358/1000 | Loss: 0.4594\n",
            "Epoch 359/1000 | Loss: 0.4593\n",
            "Epoch 360/1000 | Loss: 0.4591\n",
            "Epoch 361/1000 | Loss: 0.4590\n",
            "Epoch 362/1000 | Loss: 0.4589\n",
            "Epoch 363/1000 | Loss: 0.4587\n",
            "Epoch 364/1000 | Loss: 0.4586\n",
            "Epoch 365/1000 | Loss: 0.4584\n",
            "Epoch 366/1000 | Loss: 0.4583\n",
            "Epoch 367/1000 | Loss: 0.4582\n",
            "Epoch 368/1000 | Loss: 0.4580\n",
            "Epoch 369/1000 | Loss: 0.4579\n",
            "Epoch 370/1000 | Loss: 0.4577\n",
            "Epoch 371/1000 | Loss: 0.4576\n",
            "Epoch 372/1000 | Loss: 0.4575\n",
            "Epoch 373/1000 | Loss: 0.4573\n",
            "Epoch 374/1000 | Loss: 0.4572\n",
            "Epoch 375/1000 | Loss: 0.4570\n",
            "Epoch 376/1000 | Loss: 0.4569\n",
            "Epoch 377/1000 | Loss: 0.4568\n",
            "Epoch 378/1000 | Loss: 0.4566\n",
            "Epoch 379/1000 | Loss: 0.4565\n",
            "Epoch 380/1000 | Loss: 0.4563\n",
            "Epoch 381/1000 | Loss: 0.4562\n",
            "Epoch 382/1000 | Loss: 0.4561\n",
            "Epoch 383/1000 | Loss: 0.4559\n",
            "Epoch 384/1000 | Loss: 0.4558\n",
            "Epoch 385/1000 | Loss: 0.4557\n",
            "Epoch 386/1000 | Loss: 0.4555\n",
            "Epoch 387/1000 | Loss: 0.4554\n",
            "Epoch 388/1000 | Loss: 0.4552\n",
            "Epoch 389/1000 | Loss: 0.4551\n",
            "Epoch 390/1000 | Loss: 0.4550\n",
            "Epoch 391/1000 | Loss: 0.4548\n",
            "Epoch 392/1000 | Loss: 0.4547\n",
            "Epoch 393/1000 | Loss: 0.4546\n",
            "Epoch 394/1000 | Loss: 0.4544\n",
            "Epoch 395/1000 | Loss: 0.4543\n",
            "Epoch 396/1000 | Loss: 0.4541\n",
            "Epoch 397/1000 | Loss: 0.4540\n",
            "Epoch 398/1000 | Loss: 0.4539\n",
            "Epoch 399/1000 | Loss: 0.4537\n",
            "Epoch 400/1000 | Loss: 0.4536\n",
            "Epoch 401/1000 | Loss: 0.4535\n",
            "Epoch 402/1000 | Loss: 0.4533\n",
            "Epoch 403/1000 | Loss: 0.4532\n",
            "Epoch 404/1000 | Loss: 0.4530\n",
            "Epoch 405/1000 | Loss: 0.4529\n",
            "Epoch 406/1000 | Loss: 0.4528\n",
            "Epoch 407/1000 | Loss: 0.4526\n",
            "Epoch 408/1000 | Loss: 0.4525\n",
            "Epoch 409/1000 | Loss: 0.4524\n",
            "Epoch 410/1000 | Loss: 0.4522\n",
            "Epoch 411/1000 | Loss: 0.4521\n",
            "Epoch 412/1000 | Loss: 0.4520\n",
            "Epoch 413/1000 | Loss: 0.4518\n",
            "Epoch 414/1000 | Loss: 0.4517\n",
            "Epoch 415/1000 | Loss: 0.4516\n",
            "Epoch 416/1000 | Loss: 0.4514\n",
            "Epoch 417/1000 | Loss: 0.4513\n",
            "Epoch 418/1000 | Loss: 0.4511\n",
            "Epoch 419/1000 | Loss: 0.4510\n",
            "Epoch 420/1000 | Loss: 0.4509\n",
            "Epoch 421/1000 | Loss: 0.4507\n",
            "Epoch 422/1000 | Loss: 0.4506\n",
            "Epoch 423/1000 | Loss: 0.4505\n",
            "Epoch 424/1000 | Loss: 0.4503\n",
            "Epoch 425/1000 | Loss: 0.4502\n",
            "Epoch 426/1000 | Loss: 0.4501\n",
            "Epoch 427/1000 | Loss: 0.4499\n",
            "Epoch 428/1000 | Loss: 0.4498\n",
            "Epoch 429/1000 | Loss: 0.4497\n",
            "Epoch 430/1000 | Loss: 0.4495\n",
            "Epoch 431/1000 | Loss: 0.4494\n",
            "Epoch 432/1000 | Loss: 0.4493\n",
            "Epoch 433/1000 | Loss: 0.4491\n",
            "Epoch 434/1000 | Loss: 0.4490\n",
            "Epoch 435/1000 | Loss: 0.4489\n",
            "Epoch 436/1000 | Loss: 0.4487\n",
            "Epoch 437/1000 | Loss: 0.4486\n",
            "Epoch 438/1000 | Loss: 0.4485\n",
            "Epoch 439/1000 | Loss: 0.4483\n",
            "Epoch 440/1000 | Loss: 0.4482\n",
            "Epoch 441/1000 | Loss: 0.4481\n",
            "Epoch 442/1000 | Loss: 0.4479\n",
            "Epoch 443/1000 | Loss: 0.4478\n",
            "Epoch 444/1000 | Loss: 0.4477\n",
            "Epoch 445/1000 | Loss: 0.4475\n",
            "Epoch 446/1000 | Loss: 0.4474\n",
            "Epoch 447/1000 | Loss: 0.4473\n",
            "Epoch 448/1000 | Loss: 0.4471\n",
            "Epoch 449/1000 | Loss: 0.4470\n",
            "Epoch 450/1000 | Loss: 0.4469\n",
            "Epoch 451/1000 | Loss: 0.4467\n",
            "Epoch 452/1000 | Loss: 0.4466\n",
            "Epoch 453/1000 | Loss: 0.4465\n",
            "Epoch 454/1000 | Loss: 0.4463\n",
            "Epoch 455/1000 | Loss: 0.4462\n",
            "Epoch 456/1000 | Loss: 0.4461\n",
            "Epoch 457/1000 | Loss: 0.4459\n",
            "Epoch 458/1000 | Loss: 0.4458\n",
            "Epoch 459/1000 | Loss: 0.4457\n",
            "Epoch 460/1000 | Loss: 0.4455\n",
            "Epoch 461/1000 | Loss: 0.4454\n",
            "Epoch 462/1000 | Loss: 0.4453\n",
            "Epoch 463/1000 | Loss: 0.4451\n",
            "Epoch 464/1000 | Loss: 0.4450\n",
            "Epoch 465/1000 | Loss: 0.4449\n",
            "Epoch 466/1000 | Loss: 0.4448\n",
            "Epoch 467/1000 | Loss: 0.4446\n",
            "Epoch 468/1000 | Loss: 0.4445\n",
            "Epoch 469/1000 | Loss: 0.4444\n",
            "Epoch 470/1000 | Loss: 0.4442\n",
            "Epoch 471/1000 | Loss: 0.4441\n",
            "Epoch 472/1000 | Loss: 0.4440\n",
            "Epoch 473/1000 | Loss: 0.4438\n",
            "Epoch 474/1000 | Loss: 0.4437\n",
            "Epoch 475/1000 | Loss: 0.4436\n",
            "Epoch 476/1000 | Loss: 0.4434\n",
            "Epoch 477/1000 | Loss: 0.4433\n",
            "Epoch 478/1000 | Loss: 0.4432\n",
            "Epoch 479/1000 | Loss: 0.4431\n",
            "Epoch 480/1000 | Loss: 0.4429\n",
            "Epoch 481/1000 | Loss: 0.4428\n",
            "Epoch 482/1000 | Loss: 0.4427\n",
            "Epoch 483/1000 | Loss: 0.4425\n",
            "Epoch 484/1000 | Loss: 0.4424\n",
            "Epoch 485/1000 | Loss: 0.4423\n",
            "Epoch 486/1000 | Loss: 0.4421\n",
            "Epoch 487/1000 | Loss: 0.4420\n",
            "Epoch 488/1000 | Loss: 0.4419\n",
            "Epoch 489/1000 | Loss: 0.4418\n",
            "Epoch 490/1000 | Loss: 0.4416\n",
            "Epoch 491/1000 | Loss: 0.4415\n",
            "Epoch 492/1000 | Loss: 0.4414\n",
            "Epoch 493/1000 | Loss: 0.4412\n",
            "Epoch 494/1000 | Loss: 0.4411\n",
            "Epoch 495/1000 | Loss: 0.4410\n",
            "Epoch 496/1000 | Loss: 0.4409\n",
            "Epoch 497/1000 | Loss: 0.4407\n",
            "Epoch 498/1000 | Loss: 0.4406\n",
            "Epoch 499/1000 | Loss: 0.4405\n",
            "Epoch 500/1000 | Loss: 0.4403\n",
            "\n",
            "Let's predict the hours need to score above 50%\n",
            "==================================================\n",
            "Prediction after 1 hour of training: 0.3617 | Above 50%: False\n",
            "Prediction after 7 hours of training: 0.9758 | Above 50%: True\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}